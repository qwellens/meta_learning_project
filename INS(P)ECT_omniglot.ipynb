{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "from torch.autograd import Variable as V\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our tasks where we train on K shots of N characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniTask:\n",
    "    def __init__(self, K, N, noise_percent):\n",
    "        # K, N as in N-way K-shot learning\n",
    "        self.K = K\n",
    "        self.N = N \n",
    "        self.noise_percent = noise_percent\n",
    "        self.mini_train = None\n",
    "        self.mini_test = None\n",
    "        self.image_dir = \"omniglot_dataset/images_background/*/*\"\n",
    "        self.trainTransform  = transforms.Compose([transforms.Grayscale(num_output_channels=1),transforms.ToTensor()]) \n",
    "        self.mini_batch_size = 5\n",
    "        \n",
    "    def mini_train_set(self):\n",
    "        if self.mini_train is None:\n",
    "            #choose N tasks\n",
    "            characters = random.sample(glob.glob(self.image_dir), self.N)\n",
    "            \n",
    "            train_set = []\n",
    "            #choose K examples per task:\n",
    "            for i,char in enumerate(characters):\n",
    "                k_shots = random.sample(glob.glob(char+\"/*\"), self.K)\n",
    "                train_set.extend([(self.trainTransform(Image.open(shot).convert('RGB')), i) for shot in k_shots])\n",
    "            self.mini_train = random.sample(train_set, len(train_set))\n",
    "            \n",
    "        return self.mini_train\n",
    "    \n",
    "    def batched_mini_train_set(self):\n",
    "        train_set = self.mini_train_set()\n",
    "        shuffled = random.sample(train_set, len(train_set))\n",
    "        \n",
    "        batched = []\n",
    "        current_xes = []\n",
    "        current_yes = []\n",
    "        for i in range(len(shuffled)):\n",
    "            if (i%self.mini_batch_size==0 and i > 0) or (i == len(shuffled)-1):\n",
    "                batched.append((torch.stack(current_xes), torch.LongTensor(current_yes)))\n",
    "                current_xes = []\n",
    "                current_yes = []\n",
    "            current_xes.append(shuffled[i][0])\n",
    "            current_yes.append(shuffled[i][1])\n",
    "\n",
    "        return batched   \n",
    "    \n",
    "    def mini_test_set(self):\n",
    "        pass\n",
    "        #TODO: FIGURE OUT HOW MAML DOES THIS\n",
    "    \n",
    "    def eval_set(self, size=50):\n",
    "        pass\n",
    "        #TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = OmniTask(5,5,0)\n",
    "train_set = trial.batched_mini_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, size=50000, K = 5, N = 5, noise_percent=0):\n",
    "        self.size = size\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.noise_percent = noise_percent\n",
    "        self.tasks = None \n",
    "        \n",
    "    def generate_set(self):\n",
    "        self.tasks = tasks = [OmniTask(self.K, self.N, self.noise_percent) for _ in range(self.size)]\n",
    "        return tasks\n",
    "    \n",
    "    def shuffled_set(self):\n",
    "        if self.tasks is None:\n",
    "            self.generate_set()\n",
    "        return random.sample(self.tasks, len(self.tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # 28 x 28 - 1\n",
    "            nn.Conv2d(1, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 14 x 14 - 64\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 7 x 7 - 64\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 4 x 4 - 64\n",
    "            nn.Conv2d(64, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # 2 x 2 - 64\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 2 x 2 x 64 = 256\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.LogSoftmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv(out)\n",
    "        out = out.view(len(out), -1) # should be equivalent to out.view(-1, 256)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, prob):\n",
    "        __, argmax = prob.max(1)\n",
    "        return argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reptile Meta-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner():\n",
    "    def __init__(self, higher_order=False, lr_inner=0.01, lr_outer=0.001, sgd_steps_inner=10):\n",
    "        self.lr_inner = lr_inner\n",
    "        self.lr_outer = lr_outer\n",
    "        self.sgd_steps_inner = sgd_steps_inner\n",
    "        self.higher_order = higher_order\n",
    "        \n",
    "    def inner_train(self, model, task, optimizer):\n",
    "        batches = task.batched_mini_train_set()\n",
    "        for x,y in batches:\n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(x)\n",
    "            loss = F.nll_loss(predicted, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def init_grad(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.grad = torch.zeros_like(param)\n",
    "        \n",
    "class Reptile(MetaLearner):\n",
    "    def __init__(self, lr_inner=0.01, lr_outer=0.001, sgd_steps_inner=10):\n",
    "        super().__init__(False, lr_inner, lr_outer, sgd_steps_inner)\n",
    "        \n",
    "    def compute_store_gradients(self, target, current):\n",
    "        current_weights = dict(current.named_parameters())\n",
    "        target_weights = dict(target.named_parameters())\n",
    "        gradients = {name: (current_weights[name].data - target_weights[name].data) / (self.sgd_steps_inner * self.lr_inner) for name in target_weights}\n",
    "\n",
    "        for name in current_weights:\n",
    "            current_weights[name].grad.data = gradients[name]\n",
    "\n",
    "    def train(self, model, train_data):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.lr_outer)\n",
    "        self.init_grad(model)\n",
    "\n",
    "        for i, task in enumerate(train_data.shuffled_set()):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inner_model = copy.deepcopy(model)\n",
    "            inner_optim = torch.optim.SGD(inner_model.parameters(), lr=self.lr_inner)\n",
    "\n",
    "            for _ in range(self.sgd_steps_inner):\n",
    "                self.inner_train(inner_model, task, inner_optim)\n",
    "\n",
    "            self.compute_store_gradients(inner_model, model)\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\"iteration:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "iteration: 100\n",
      "iteration: 200\n",
      "iteration: 300\n",
      "iteration: 400\n",
      "iteration: 500\n",
      "iteration: 600\n",
      "iteration: 700\n",
      "iteration: 800\n",
      "iteration: 900\n",
      "iteration: 1000\n",
      "iteration: 1100\n",
      "iteration: 1200\n",
      "iteration: 1300\n",
      "iteration: 1400\n",
      "iteration: 1500\n",
      "iteration: 1600\n",
      "iteration: 1700\n",
      "iteration: 1800\n",
      "iteration: 1900\n",
      "iteration: 2000\n",
      "iteration: 2100\n",
      "iteration: 2200\n",
      "iteration: 2300\n",
      "iteration: 2400\n",
      "iteration: 2500\n",
      "iteration: 2600\n"
     ]
    }
   ],
   "source": [
    "reptile_model = OmniglotNet(num_classes=5)\n",
    "reptile_learning_alg = Reptile()\n",
    "train_data = DataGenerator()\n",
    "reptile_learning_alg.train(reptile_model, train_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
